# Twilio WebSocket Audio Processor
This Rust application demonstrates handling, processing, and playing back Twilio phone call audio data received over a live duplex WebSocket connection. Utilizing the `warp` web server framework for asynchronous WebSocket communication, the application efficiently processes batches of audio data encoded in base64 format. Key functionalities include audio data queue management, batch processing for efficiency, raw audio file creation, format conversion (from uLaw to WAV), and playback of base64-encoded audio chunks.

# Documentation for Speech Detection in Audio Streams

This document provides an overview of a multi-level algorithm designed to detect speech and pauses within an audio stream. The algorithm is structured to first identify speech and pauses based on amplitude thresholds and then determine the start and end of speech sequences for further processing, such as voice-to-text conversion.

## Overview

The speech detection system operates in two main stages:

- **Level 1**: Identification of speech and pauses based on amplitude thresholds.
- **Level 2**: Determination of the start and end points of speech clips based on the frequency of detected speech and pauses.

## Level 1: Speech and Pause Detection

### Speech Detection

Speech is detected by analyzing the amplitude of the audio stream. When the amplitude consecutively exceeds a predefined threshold (`SPEACH_DETECTION_AMPLITUDE_THRESHOLD`) for a specified number of samples (`SPEACH_DETECTION_SEQUENCE_THRESHOLD`), speech is considered to be detected.

- **Amplitude Threshold for Speech Detection**: 5000 (16-bit signed integer)
- **Sequence Threshold for Speech Detection**: 5 samples

### Pause Detection

Similarly, pauses within the audio stream are identified by monitoring for amplitudes that consecutively fall below a lower threshold (`PAUSE_DETECTION_AMPLITUDE_THRESHOLD`) for a longer sequence of samples (`PAUSE_DETECTION_SEQUENCE_THRESHOLD`).

- **Amplitude Threshold for Pause Detection**: 2000 (16-bit signed integer)
- **Sequence Threshold for Pause Detection**: 15 samples

## Level 2: Speech Clip Detection

Once speech and pauses are identified, the system uses these detections to mark the beginning and end of speech clips. This is achieved by counting consecutive speech and pause detections and applying minimum thresholds for both.

- **Minimum Consecutive Speech Count**: 5
- **Minimum Consecutive Pause Count**: 3
- **Offset for Sequence Number Adjustment**: 10

This approach allows the system to dynamically adjust to varying audio conditions and ensure accurate speech clip boundaries.

## Implementation Details

### Amplitude Calculation

The amplitude of audio samples is calculated from Âµ-law encoded bytes converted to linear PCM values. This conversion ensures a wide dynamic range suitable for detecting speech in diverse audio conditions.

### Concurrency and State Management

The algorithm leverages asynchronous programming to perform amplitude calculation and message queue management in parallel, ensuring efficient processing of live audio streams. State management is critical, with different states (`Idle`, `InSpeech`, `PostSpeech`) dictating the flow of detection and processing.

### Detection Algorithms

Both speech and pause detection algorithms utilize a sliding window approach, maintaining a queue of recent amplitudes and checking if they meet the criteria for detection. Upon detection, the relevant queues are cleared to reset the detection state, readying the system for subsequent audio analysis.

## Processing Flow

1. **Amplitude Calculation and Message Insertion**: Performed in parallel, this step calculates the peak amplitude of an audio chunk while asynchronously inserting messages into a queue for processing.
2. **Speech and Pause Detection**: Also executed in parallel, these steps analyze the calculated amplitude to detect speech or pauses according to their respective thresholds and sequence criteria.
3. **Start and End Detection**: Based on the outcomes of speech and pause detection, the system transitions between states (`Idle`, `InSpeech`, `PostSpeech`) to mark the start and end of speech clips. Adjustments are made using an offset to accurately capture the speech boundaries.

This comprehensive approach ensures accurate detection of speech within audio streams, facilitating further processing such as voice recognition and transcription.

![image](https://github.com/stefonalfaro/twilio-websocket-to-audio-wav/assets/45152948/1b85ce40-6f67-4137-880c-1d844fa389ad)

## Features

- **WebSocket Communication**: Leverages `warp` for efficient WebSocket handling.
- **Audio Queue Management**: Utilizes a global, thread-safe queue to manage incoming audio data.
- **Batch Processing**: Processes audio data in batches to optimize performance.
- **Audio Conversion**: Converts raw uLaw audio files to WAV format using `ffmpeg`.
- **Voice Detection**: Detects the sequence range of messages that contains speech.

## Installation

Ensure you have Rust and Cargo installed on your system. Additionally, `ffmpeg` must be installed and accessible in your system's PATH for audio conversion.


## Dependencies

- [warp](https://crates.io/crates/warp): For WebSocket and web server functionality.
- [tokio](https://crates.io/crates/tokio): Asynchronous runtime.
- [rodio](https://crates.io/crates/rodio): For audio playback.
- [serde_json](https://crates.io/crates/serde_json): For JSON parsing.
- [lazy_static](https://crates.io/crates/lazy_static): For defining lazy-initialized static variables.
- [log](https://crates.io/crates/log): For logging.
